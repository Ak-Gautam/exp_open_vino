{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ak-Gautam/exp_open_vino/blob/main/open_vino_llm_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5df233b0-0369-4fff-9952-7957a90394a5",
      "metadata": {
        "id": "5df233b0-0369-4fff-9952-7957a90394a5"
      },
      "source": [
        "## Prerequisites\n",
        "[back to top ⬆️](#Table-of-contents:)\n",
        "\n",
        "Install required dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "563ecf9f-346b-4f14-85ef-c66ff0c95f65",
      "metadata": {
        "tags": [],
        "id": "563ecf9f-346b-4f14-85ef-c66ff0c95f65",
        "outputId": "627a6588-fd7f-4256-e5d1-2399425e7ae3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/2.1 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Skipping optimum as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping optimum-intel as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.0/39.0 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.4/68.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.1/318.1 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m418.4/418.4 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.1/249.1 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for optimum-intel (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for nncf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transformers_stream_generator (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for jstyleson (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for grapheme (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"GIT_CLONE_PROTECTION_ACTIVE\"] = \"false\"\n",
        "\n",
        "%pip install -Uq pip\n",
        "%pip uninstall -q -y optimum optimum-intel\n",
        "%pip install --pre -Uq openvino openvino-tokenizers[transformers] --extra-index-url https://storage.openvinotoolkit.org/simple/wheels/nightly\n",
        "%pip install -q --extra-index-url https://download.pytorch.org/whl/cpu\\\n",
        "\"git+https://github.com/huggingface/optimum-intel.git\"\\\n",
        "\"git+https://github.com/openvinotoolkit/nncf.git\"\\\n",
        "\"torch>=2.1\"\\\n",
        "\"datasets\" \\\n",
        "\"accelerate\"\\\n",
        "\"gradio>=4.19\"\\\n",
        "\"onnx\" \"einops\" \"transformers_stream_generator\" \"tiktoken\" \"transformers>=4.40\" \"bitsandbytes\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import requests\n",
        "import shutil\n",
        "\n",
        "# fetch model configuration\n",
        "\n",
        "config_shared_path = Path(\"../../utils/llm_config.py\")\n",
        "config_dst_path = Path(\"llm_config.py\")\n",
        "\n",
        "if not config_dst_path.exists():\n",
        "    if config_shared_path.exists():\n",
        "        try:\n",
        "            os.symlink(config_shared_path, config_dst_path)\n",
        "        except Exception:\n",
        "            shutil.copy(config_shared_path, config_dst_path)\n",
        "    else:\n",
        "        r = requests.get(url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/llm_config.py\")\n",
        "        with open(\"llm_config.py\", \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(r.text)\n",
        "elif not os.path.islink(config_dst_path):\n",
        "    print(\"LLM config will be updated\")\n",
        "    if config_shared_path.exists():\n",
        "        shutil.copy(config_shared_path, config_dst_path)\n",
        "    else:\n",
        "        r = requests.get(url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/llm_config.py\")\n",
        "        with open(\"llm_config.py\", \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(r.text)"
      ],
      "metadata": {
        "id": "3X5lPc-gbn6s"
      },
      "id": "3X5lPc-gbn6s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llm_config import SUPPORTED_LLM_MODELS\n",
        "import ipywidgets as widgets"
      ],
      "metadata": {
        "id": "KtUn5GF-brEj"
      },
      "id": "KtUn5GF-brEj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Choose English here"
      ],
      "metadata": {
        "id": "VLpQnqpVglID"
      },
      "id": "VLpQnqpVglID"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e02b34fb",
      "metadata": {
        "id": "e02b34fb",
        "outputId": "0683857b-a71d-449a-b00f-dd1866b38ac9",
        "colab": {
          "referenced_widgets": [
            "66dd97760edb4f7294b940ce484064eb",
            "11521864b77e4bdfb86e441e5e75ceb7",
            "e55dd0c3659a441e802913c067e94488"
          ],
          "base_uri": "https://localhost:8080/",
          "height": 49
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dropdown(description='Model Language:', options=('English', 'Chinese', 'Japanese'), value='English')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "66dd97760edb4f7294b940ce484064eb"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "model_languages = list(SUPPORTED_LLM_MODELS)\n",
        "\n",
        "model_language = widgets.Dropdown(\n",
        "    options=model_languages,\n",
        "    value=model_languages[0],\n",
        "    description=\"Model Language:\",\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "model_language"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4EU2At1kgpAS"
      },
      "id": "4EU2At1kgpAS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Choose qwen2-1.5b-instruct here"
      ],
      "metadata": {
        "id": "X4T1pUMUgvwY"
      },
      "id": "X4T1pUMUgvwY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d22fedb-d1f6-4306-b910-efac5b849c7c",
      "metadata": {
        "tags": [],
        "id": "8d22fedb-d1f6-4306-b910-efac5b849c7c",
        "outputId": "8e3a8c75-bf81-414f-918b-66a0b93e6e95",
        "colab": {
          "referenced_widgets": [
            "31baeee064c24593b73ff90e0efa7220",
            "2dd331850c354933840ddc2552443c57",
            "0b97212db3b54a618e91bdba71b4fccb"
          ],
          "base_uri": "https://localhost:8080/",
          "height": 49
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dropdown(description='Model:', options=('qwen2-0.5b-instruct', 'tiny-llama-1b-chat', 'qwen2-1.5b-instruct', 'g…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "31baeee064c24593b73ff90e0efa7220"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "model_ids = list(SUPPORTED_LLM_MODELS[model_language.value])\n",
        "\n",
        "model_id = widgets.Dropdown(\n",
        "    options=model_ids,\n",
        "    value=model_ids[0],\n",
        "    description=\"Model:\",\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "model_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "906022ec-96bf-41a9-9447-789d2e875250",
      "metadata": {
        "tags": [],
        "id": "906022ec-96bf-41a9-9447-789d2e875250",
        "outputId": "73a1ad9c-30a8-4a56-e300-388c2c51c55d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected model qwen2-1.5b-instruct\n"
          ]
        }
      ],
      "source": [
        "model_configuration = SUPPORTED_LLM_MODELS[model_language.value][model_id.value]\n",
        "print(f\"Selected model {model_id.value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Only tick fp16 here"
      ],
      "metadata": {
        "id": "D01tsJ8Qg2lA"
      },
      "id": "D01tsJ8Qg2lA"
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown, display\n",
        "\n",
        "prepare_int4_model = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description=\"Prepare INT4 model\",\n",
        "    disabled=False,\n",
        ")\n",
        "prepare_int8_model = widgets.Checkbox(\n",
        "    value=False,\n",
        "    description=\"Prepare INT8 model\",\n",
        "    disabled=False,\n",
        ")\n",
        "prepare_fp16_model = widgets.Checkbox(\n",
        "    value=False,\n",
        "    description=\"Prepare FP16 model\",\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "display(prepare_int4_model)\n",
        "display(prepare_int8_model)\n",
        "display(prepare_fp16_model)"
      ],
      "metadata": {
        "id": "dJ5gNvfIcO5y",
        "outputId": "22c8bbe7-2767-4a12-d40f-1057999c4c38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109,
          "referenced_widgets": [
            "fc0964db45b74e7d80187c5d46ec76a6",
            "e306b160f6074a5c9901e472f0ea0e4a",
            "4ca07588c92e4c93aede06cd2460818f",
            "e92fab4be5f74c0198ebf8951d541174",
            "fe3ef60bd2064968a85edb51e832fd44",
            "f99518bbe9494318817749189068ef11",
            "2a4d2ca2fcca46f299063d2bd83c4fc5",
            "362e1881b74d46d3b31a045a86b60d83",
            "cf03cc071a7546cb8761000bfa123b4c"
          ]
        }
      },
      "id": "dJ5gNvfIcO5y",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Checkbox(value=True, description='Prepare INT4 model')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fc0964db45b74e7d80187c5d46ec76a6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Checkbox(value=False, description='Prepare INT8 model')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e92fab4be5f74c0198ebf8951d541174"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Checkbox(value=False, description='Prepare FP16 model')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2a4d2ca2fcca46f299063d2bd83c4fc5"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "130a037a-7d98-4152-81ea-92ffb01da5a2",
      "metadata": {
        "id": "130a037a-7d98-4152-81ea-92ffb01da5a2"
      },
      "source": [
        "We can now save floating point and compressed model variants"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Don't tick, leave it empty."
      ],
      "metadata": {
        "id": "d3Bu_GwVg67v"
      },
      "id": "d3Bu_GwVg67v"
    },
    {
      "cell_type": "code",
      "source": [
        "enable_awq = widgets.Checkbox(\n",
        "    value=False,\n",
        "    description=\"Enable AWQ\",\n",
        "    disabled=not prepare_int4_model.value,\n",
        ")\n",
        "display(enable_awq)"
      ],
      "metadata": {
        "id": "0vF6umdec2Zc",
        "outputId": "fb9d9dd9-cf8b-486c-df49-e07254e55eba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "b079b23f0fd94df089ab7b3687bd5476",
            "88fd63e0f8ed4e6a88e7f77296a2b32a",
            "fdc11a0fdd4c42d5bf5018f6603e2421"
          ]
        }
      },
      "id": "0vF6umdec2Zc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Checkbox(value=False, description='Enable AWQ', disabled=True)"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b079b23f0fd94df089ab7b3687bd5476"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This will take some time (10 - 15 mins) so be patient"
      ],
      "metadata": {
        "id": "pm-XCpTehDCQ"
      },
      "id": "pm-XCpTehDCQ"
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "pt_model_id = model_configuration[\"model_id\"]\n",
        "pt_model_name = model_id.value.split(\"-\")[0]\n",
        "fp16_model_dir = Path(model_id.value) / \"FP16\"\n",
        "int8_model_dir = Path(model_id.value) / \"INT8_compressed_weights\"\n",
        "int4_model_dir = Path(model_id.value) / \"INT4_compressed_weights\"\n",
        "\n",
        "\n",
        "def convert_to_fp16():\n",
        "    if (fp16_model_dir / \"openvino_model.xml\").exists():\n",
        "        return\n",
        "    remote_code = model_configuration.get(\"remote_code\", False)\n",
        "    export_command_base = \"optimum-cli export openvino --model {} --task text-generation-with-past --weight-format fp16\".format(pt_model_id)\n",
        "    if remote_code:\n",
        "        export_command_base += \" --trust-remote-code\"\n",
        "    export_command = export_command_base + \" \" + str(fp16_model_dir)\n",
        "    display(Markdown(\"**Export command:**\"))\n",
        "    display(Markdown(f\"`{export_command}`\"))\n",
        "    ! $export_command\n",
        "\n",
        "\n",
        "def convert_to_int8():\n",
        "    if (int8_model_dir / \"openvino_model.xml\").exists():\n",
        "        return\n",
        "    int8_model_dir.mkdir(parents=True, exist_ok=True)\n",
        "    remote_code = model_configuration.get(\"remote_code\", False)\n",
        "    export_command_base = \"optimum-cli export openvino --model {} --task text-generation-with-past --weight-format int8\".format(pt_model_id)\n",
        "    if remote_code:\n",
        "        export_command_base += \" --trust-remote-code\"\n",
        "    export_command = export_command_base + \" \" + str(int8_model_dir)\n",
        "    display(Markdown(\"**Export command:**\"))\n",
        "    display(Markdown(f\"`{export_command}`\"))\n",
        "    ! $export_command\n",
        "\n",
        "\n",
        "def convert_to_int4():\n",
        "    compression_configs = {\n",
        "        \"zephyr-7b-beta\": {\n",
        "            \"sym\": True,\n",
        "            \"group_size\": 64,\n",
        "            \"ratio\": 0.6,\n",
        "        },\n",
        "        \"mistral-7b\": {\n",
        "            \"sym\": True,\n",
        "            \"group_size\": 64,\n",
        "            \"ratio\": 0.6,\n",
        "        },\n",
        "        \"minicpm-2b-dpo\": {\n",
        "            \"sym\": True,\n",
        "            \"group_size\": 64,\n",
        "            \"ratio\": 0.6,\n",
        "        },\n",
        "        \"gemma-2b-it\": {\n",
        "            \"sym\": True,\n",
        "            \"group_size\": 64,\n",
        "            \"ratio\": 0.6,\n",
        "        },\n",
        "        \"notus-7b-v1\": {\n",
        "            \"sym\": True,\n",
        "            \"group_size\": 64,\n",
        "            \"ratio\": 0.6,\n",
        "        },\n",
        "        \"neural-chat-7b-v3-1\": {\n",
        "            \"sym\": True,\n",
        "            \"group_size\": 64,\n",
        "            \"ratio\": 0.6,\n",
        "        },\n",
        "        \"llama-2-chat-7b\": {\n",
        "            \"sym\": True,\n",
        "            \"group_size\": 128,\n",
        "            \"ratio\": 0.8,\n",
        "        },\n",
        "        \"llama-3-8b-instruct\": {\n",
        "            \"sym\": True,\n",
        "            \"group_size\": 128,\n",
        "            \"ratio\": 0.8,\n",
        "        },\n",
        "        \"gemma-7b-it\": {\n",
        "            \"sym\": True,\n",
        "            \"group_size\": 128,\n",
        "            \"ratio\": 0.8,\n",
        "        },\n",
        "        \"chatglm2-6b\": {\n",
        "            \"sym\": True,\n",
        "            \"group_size\": 128,\n",
        "            \"ratio\": 0.72,\n",
        "        },\n",
        "        \"qwen-7b-chat\": {\"sym\": True, \"group_size\": 128, \"ratio\": 0.6},\n",
        "        \"red-pajama-3b-chat\": {\n",
        "            \"sym\": False,\n",
        "            \"group_size\": 128,\n",
        "            \"ratio\": 0.5,\n",
        "        },\n",
        "        \"default\": {\n",
        "            \"sym\": False,\n",
        "            \"group_size\": 128,\n",
        "            \"ratio\": 0.8,\n",
        "        },\n",
        "    }\n",
        "\n",
        "    model_compression_params = compression_configs.get(model_id.value, compression_configs[\"default\"])\n",
        "    if (int4_model_dir / \"openvino_model.xml\").exists():\n",
        "        return\n",
        "    remote_code = model_configuration.get(\"remote_code\", False)\n",
        "    export_command_base = \"optimum-cli export openvino --model {} --task text-generation-with-past --weight-format int4\".format(pt_model_id)\n",
        "    int4_compression_args = \" --group-size {} --ratio {}\".format(model_compression_params[\"group_size\"], model_compression_params[\"ratio\"])\n",
        "    if model_compression_params[\"sym\"]:\n",
        "        int4_compression_args += \" --sym\"\n",
        "    if enable_awq.value:\n",
        "        int4_compression_args += \" --awq --dataset wikitext2 --num-samples 128\"\n",
        "    export_command_base += int4_compression_args\n",
        "    if remote_code:\n",
        "        export_command_base += \" --trust-remote-code\"\n",
        "    export_command = export_command_base + \" \" + str(int4_model_dir)\n",
        "    display(Markdown(\"**Export command:**\"))\n",
        "    display(Markdown(f\"`{export_command}`\"))\n",
        "    ! $export_command\n",
        "\n",
        "\n",
        "if prepare_fp16_model.value:\n",
        "    convert_to_fp16()\n",
        "if prepare_int8_model.value:\n",
        "    convert_to_int8()\n",
        "if prepare_int4_model.value:\n",
        "    convert_to_int4()"
      ],
      "metadata": {
        "id": "odAq4i8hdMxq",
        "outputId": "a6dc4ff0-6f33-450c-bc36-56ab0d451dc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        }
      },
      "id": "odAq4i8hdMxq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Export command:**"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "`optimum-cli export openvino --model Qwen/Qwen2-1.5B-Instruct --task text-generation-with-past --weight-format fp16 qwen2-1.5b-instruct/FP16`"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-06-15 15:12:34.396260: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-06-15 15:12:34.396366: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-06-15 15:12:34.399800: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-06-15 15:12:36.037979: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
            "config.json: 100% 660/660 [00:00<00:00, 3.16MB/s]\n",
            "Framework not specified. Using pt to export the model.\n",
            "model.safetensors: 100% 3.09G/3.09G [00:34<00:00, 89.5MB/s]\n",
            "generation_config.json: 100% 242/242 [00:00<00:00, 864kB/s]\n",
            "tokenizer_config.json: 100% 1.29k/1.29k [00:00<00:00, 2.46MB/s]\n",
            "vocab.json: 100% 2.78M/2.78M [00:00<00:00, 9.12MB/s]\n",
            "merges.txt: 100% 1.67M/1.67M [00:00<00:00, 5.21MB/s]\n",
            "tokenizer.json: 100% 7.03M/7.03M [00:00<00:00, 28.4MB/s]\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Using framework PyTorch: 2.3.0+cu121\n",
            "Overriding 1 configuration item(s)\n",
            "\t- use_cache -> True\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_attn_mask_utils.py:114: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if (input_shape[-1] > 1 or self.sliding_window is not None) and self.is_causal:\n",
            "/usr/local/lib/python3.10/dist-packages/optimum/exporters/onnx/model_patcher.py:300: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if past_key_values_length > 0:\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2/modeling_qwen2.py:120: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if seq_len > self.max_seq_len_cached:\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2/modeling_qwen2.py:667: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n",
            "Replacing `(?!\\S)` pattern to `(?:$|[^\\S])` in RegexSplit operation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fp16_weights = fp16_model_dir / \"openvino_model.bin\"\n",
        "int8_weights = int8_model_dir / \"openvino_model.bin\"\n",
        "int4_weights = int4_model_dir / \"openvino_model.bin\"\n",
        "\n",
        "if fp16_weights.exists():\n",
        "    print(f\"Size of FP16 model is {fp16_weights.stat().st_size / 1024 / 1024:.2f} MB\")\n",
        "for precision, compressed_weights in zip([8, 4], [int8_weights, int4_weights]):\n",
        "    if compressed_weights.exists():\n",
        "        print(f\"Size of model with INT{precision} compressed weights is {compressed_weights.stat().st_size / 1024 / 1024:.2f} MB\")\n",
        "    if compressed_weights.exists() and fp16_weights.exists():\n",
        "        print(f\"Compression rate for INT{precision} model: {fp16_weights.stat().st_size / compressed_weights.stat().st_size:.3f}\")"
      ],
      "metadata": {
        "id": "M09MuNNbeWux",
        "outputId": "83e58d25-ae28-42ae-854a-b48b4e80a872",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "M09MuNNbeWux",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of FP16 model is 2960.40 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "671a17d4",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "671a17d4"
      },
      "source": [
        "Let's compare model size for different compression types"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d62f9f4-5434-4550-b372-c86b5a5089d5",
      "metadata": {
        "id": "6d62f9f4-5434-4550-b372-c86b5a5089d5"
      },
      "source": [
        "## Select device for inference and model variant\n",
        "[back to top ⬆️](#Table-of-contents:)\n",
        "\n",
        ">**Note**: There may be no speedup for INT4/INT8 compressed models on dGPU."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Select CPU (default)"
      ],
      "metadata": {
        "id": "P6P0uDPChMWA"
      },
      "id": "P6P0uDPChMWA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "837b4a3b-ccc3-4004-9577-2b2c7b802dea",
      "metadata": {
        "tags": [],
        "id": "837b4a3b-ccc3-4004-9577-2b2c7b802dea",
        "outputId": "6d3165d1-fb4f-4735-ee0c-5c1ef4716b44",
        "colab": {
          "referenced_widgets": [
            "7685bb0f3ebc479eb05b0cf05617f761",
            "c803bca6d5b14e86a473c999805391ed",
            "f1f7e4310ce14928abfe1d9c05187a71"
          ],
          "base_uri": "https://localhost:8080/",
          "height": 49
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dropdown(description='Device:', options=('CPU', 'AUTO'), value='CPU')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7685bb0f3ebc479eb05b0cf05617f761"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import openvino as ov\n",
        "\n",
        "core = ov.Core()\n",
        "\n",
        "support_devices = core.available_devices\n",
        "if \"NPU\" in support_devices:\n",
        "    support_devices.remove(\"NPU\")\n",
        "\n",
        "device = widgets.Dropdown(\n",
        "    options=support_devices + [\"AUTO\"],\n",
        "    value=\"CPU\",\n",
        "    description=\"Device:\",\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-R_DAnB8hQUR"
      },
      "id": "-R_DAnB8hQUR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Selct FP16 here (default)"
      ],
      "metadata": {
        "id": "pPQyMp8mhYi4"
      },
      "id": "pPQyMp8mhYi4"
    },
    {
      "cell_type": "code",
      "source": [
        "available_models = []\n",
        "if int4_model_dir.exists():\n",
        "    available_models.append(\"INT4\")\n",
        "if int8_model_dir.exists():\n",
        "    available_models.append(\"INT8\")\n",
        "if fp16_model_dir.exists():\n",
        "    available_models.append(\"FP16\")\n",
        "\n",
        "model_to_run = widgets.Dropdown(\n",
        "    options=available_models,\n",
        "    value=available_models[0],\n",
        "    description=\"Model to run:\",\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "model_to_run"
      ],
      "metadata": {
        "id": "zLmj0PYCemA5",
        "outputId": "3c760f7a-74f0-4598-8a7c-ccc08ac141b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "0010de2f731d4ce8af44bc6076fbb985",
            "05f08b5b73494487990d74e1b7856af2",
            "4eeac02700bd4b1a802b3731a82f5b1a"
          ]
        }
      },
      "id": "zLmj0PYCemA5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dropdown(description='Model to run:', options=('FP16',), value='FP16')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0010de2f731d4ce8af44bc6076fbb985"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd55ade7-0445-47e1-90f0-72b82da016ca",
      "metadata": {
        "id": "bd55ade7-0445-47e1-90f0-72b82da016ca"
      },
      "source": [
        "The cell below create `OVMPTModel` and `OVCHATGLM2Model` wrapper based on `OVModelForCausalLM` model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c53001e7-615f-4eb5-b831-4e2b2ff32826",
      "metadata": {
        "tags": [],
        "id": "c53001e7-615f-4eb5-b831-4e2b2ff32826"
      },
      "source": [
        "The cell below demonstrates how to instantiate model based on selected variant of model weights and inference device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a041101-7336-40fd-96c9-cd298015a0f3",
      "metadata": {
        "tags": [],
        "id": "7a041101-7336-40fd-96c9-cd298015a0f3",
        "outputId": "c9a1533f-4f2f-4e26-c223-5f31b0d28553",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from qwen2-1.5b-instruct/FP16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "The argument `trust_remote_code` is to be used along with export=True. It will be ignored.\n",
            "Compiling the model to CPU ...\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoConfig, AutoTokenizer\n",
        "from optimum.intel.openvino import OVModelForCausalLM\n",
        "\n",
        "if model_to_run.value == \"INT4\":\n",
        "    model_dir = int4_model_dir\n",
        "elif model_to_run.value == \"INT8\":\n",
        "    model_dir = int8_model_dir\n",
        "else:\n",
        "    model_dir = fp16_model_dir\n",
        "print(f\"Loading model from {model_dir}\")\n",
        "\n",
        "ov_config = {\"PERFORMANCE_HINT\": \"LATENCY\", \"NUM_STREAMS\": \"1\", \"CACHE_DIR\": \"\"}\n",
        "\n",
        "if \"GPU\" in device.value and \"qwen2-7b-instruct\" in model_id.value:\n",
        "    ov_config[\"GPU_ENABLE_SDPA_OPTIMIZATION\"] = \"NO\"\n",
        "\n",
        "# On a GPU device a model is executed in FP16 precision. For red-pajama-3b-chat model there known accuracy\n",
        "# issues caused by this, which we avoid by setting precision hint to \"f32\".\n",
        "if model_id.value == \"red-pajama-3b-chat\" and \"GPU\" in core.available_devices and device.value in [\"GPU\", \"AUTO\"]:\n",
        "    ov_config[\"INFERENCE_PRECISION_HINT\"] = \"f32\"\n",
        "\n",
        "model_name = model_configuration[\"model_id\"]\n",
        "tok = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
        "\n",
        "ov_model = OVModelForCausalLM.from_pretrained(\n",
        "    model_dir,\n",
        "    device=device.value,\n",
        "    ov_config=ov_config,\n",
        "    config=AutoConfig.from_pretrained(model_dir, trust_remote_code=True),\n",
        "    trust_remote_code=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f6f7596-5677-4931-875b-aaabfa23cabc",
      "metadata": {
        "id": "8f6f7596-5677-4931-875b-aaabfa23cabc",
        "outputId": "b21329f2-abe0-4360-8880-a146b137ba81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 + 2 = 4\n"
          ]
        }
      ],
      "source": [
        "tokenizer_kwargs = model_configuration.get(\"tokenizer_kwargs\", {})\n",
        "test_string = \"2 + 2 =\"\n",
        "input_tokens = tok(test_string, return_tensors=\"pt\", **tokenizer_kwargs)\n",
        "answer = ov_model.generate(**input_tokens, max_new_tokens=2)\n",
        "print(tok.batch_decode(answer, skip_special_tokens=True)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This will again take some time (2 - 5 mins)"
      ],
      "metadata": {
        "id": "JvIb3JDCheOP"
      },
      "id": "JvIb3JDCheOP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01f8f7f8-072e-45dc-b7c9-18d8c3c47754",
      "metadata": {
        "tags": [],
        "id": "01f8f7f8-072e-45dc-b7c9-18d8c3c47754"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from threading import Event, Thread\n",
        "from uuid import uuid4\n",
        "from typing import List, Tuple\n",
        "import gradio as gr\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    StoppingCriteria,\n",
        "    StoppingCriteriaList,\n",
        "    TextIteratorStreamer,\n",
        ")\n",
        "\n",
        "\n",
        "model_name = model_configuration[\"model_id\"]\n",
        "start_message = model_configuration[\"start_message\"]\n",
        "history_template = model_configuration.get(\"history_template\")\n",
        "current_message_template = model_configuration.get(\"current_message_template\")\n",
        "stop_tokens = model_configuration.get(\"stop_tokens\")\n",
        "tokenizer_kwargs = model_configuration.get(\"tokenizer_kwargs\", {})\n",
        "\n",
        "english_examples = [\n",
        "    [\"Hello there! How are you doing?\"],\n",
        "    [\"What is OpenVINO?\"],\n",
        "    [\"Who are you?\"],\n",
        "    [\"Can you explain to me briefly what is Python programming language?\"],\n",
        "    [\"Explain the plot of Cinderella in a sentence.\"],\n",
        "    [\"What are some common mistakes to avoid when writing code?\"],\n",
        "    [\"Write a 100-word blog post on “Benefits of Artificial Intelligence and OpenVINO“\"],\n",
        "]\n",
        "\n",
        "examples = chinese_examples if (model_language.value == \"Chinese\") else japanese_examples if (model_language.value == \"Japanese\") else english_examples\n",
        "\n",
        "max_new_tokens = 256\n",
        "\n",
        "class StopOnTokens(StoppingCriteria):\n",
        "    def __init__(self, token_ids):\n",
        "        self.token_ids = token_ids\n",
        "\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
        "        for stop_id in self.token_ids:\n",
        "            if input_ids[0][-1] == stop_id:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "\n",
        "if stop_tokens is not None:\n",
        "    if isinstance(stop_tokens[0], str):\n",
        "        stop_tokens = tok.convert_tokens_to_ids(stop_tokens)\n",
        "\n",
        "    stop_tokens = [StopOnTokens(stop_tokens)]\n",
        "\n",
        "\n",
        "def default_partial_text_processor(partial_text: str, new_text: str):\n",
        "    \"\"\"\n",
        "    helper for updating partially generated answer, used by default\n",
        "\n",
        "    Params:\n",
        "      partial_text: text buffer for storing previosly generated text\n",
        "      new_text: text update for the current step\n",
        "    Returns:\n",
        "      updated text string\n",
        "\n",
        "    \"\"\"\n",
        "    partial_text += new_text\n",
        "    return partial_text\n",
        "\n",
        "\n",
        "text_processor = model_configuration.get(\"partial_text_processor\", default_partial_text_processor)\n",
        "\n",
        "\n",
        "def convert_history_to_token(history: List[Tuple[str, str]]):\n",
        "    \"\"\"\n",
        "    function for conversion history stored as list pairs of user and assistant messages to tokens according to model expected conversation template\n",
        "    Params:\n",
        "      history: dialogue history\n",
        "    Returns:\n",
        "      history in token format\n",
        "    \"\"\"\n",
        "    if pt_model_name == \"baichuan2\":\n",
        "        system_tokens = tok.encode(start_message)\n",
        "        history_tokens = []\n",
        "        for old_query, response in history[:-1]:\n",
        "            round_tokens = []\n",
        "            round_tokens.append(195)\n",
        "            round_tokens.extend(tok.encode(old_query))\n",
        "            round_tokens.append(196)\n",
        "            round_tokens.extend(tok.encode(response))\n",
        "            history_tokens = round_tokens + history_tokens\n",
        "        input_tokens = system_tokens + history_tokens\n",
        "        input_tokens.append(195)\n",
        "        input_tokens.extend(tok.encode(history[-1][0]))\n",
        "        input_tokens.append(196)\n",
        "        input_token = torch.LongTensor([input_tokens])\n",
        "    elif history_template is None:\n",
        "        messages = [{\"role\": \"system\", \"content\": start_message}]\n",
        "        for idx, (user_msg, model_msg) in enumerate(history):\n",
        "            if idx == len(history) - 1 and not model_msg:\n",
        "                messages.append({\"role\": \"user\", \"content\": user_msg})\n",
        "                break\n",
        "            if user_msg:\n",
        "                messages.append({\"role\": \"user\", \"content\": user_msg})\n",
        "            if model_msg:\n",
        "                messages.append({\"role\": \"assistant\", \"content\": model_msg})\n",
        "\n",
        "        input_token = tok.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_tensors=\"pt\")\n",
        "    else:\n",
        "        text = start_message + \"\".join(\n",
        "            [\"\".join([history_template.format(num=round, user=item[0], assistant=item[1])]) for round, item in enumerate(history[:-1])]\n",
        "        )\n",
        "        text += \"\".join(\n",
        "            [\n",
        "                \"\".join(\n",
        "                    [\n",
        "                        current_message_template.format(\n",
        "                            num=len(history) + 1,\n",
        "                            user=history[-1][0],\n",
        "                            assistant=history[-1][1],\n",
        "                        )\n",
        "                    ]\n",
        "                )\n",
        "            ]\n",
        "        )\n",
        "        input_token = tok(text, return_tensors=\"pt\", **tokenizer_kwargs).input_ids\n",
        "    return input_token\n",
        "\n",
        "\n",
        "def user(message, history):\n",
        "    \"\"\"\n",
        "    callback function for updating user messages in interface on submit button click\n",
        "\n",
        "    Params:\n",
        "      message: current message\n",
        "      history: conversation history\n",
        "    Returns:\n",
        "      None\n",
        "    \"\"\"\n",
        "    # Append the user's message to the conversation history\n",
        "    return \"\", history + [[message, \"\"]]\n",
        "\n",
        "\n",
        "def bot(history, temperature, top_p, top_k, repetition_penalty, conversation_id):\n",
        "    \"\"\"\n",
        "    callback function for running chatbot on submit button click\n",
        "\n",
        "    Params:\n",
        "      history: conversation history\n",
        "      temperature:  parameter for control the level of creativity in AI-generated text.\n",
        "                    By adjusting the `temperature`, you can influence the AI model's probability distribution, making the text more focused or diverse.\n",
        "      top_p: parameter for control the range of tokens considered by the AI model based on their cumulative probability.\n",
        "      top_k: parameter for control the range of tokens considered by the AI model based on their cumulative probability, selecting number of tokens with highest probability.\n",
        "      repetition_penalty: parameter for penalizing tokens based on how frequently they occur in the text.\n",
        "      conversation_id: unique conversation identifier.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Construct the input message string for the model by concatenating the current system message and conversation history\n",
        "    # Tokenize the messages string\n",
        "    input_ids = convert_history_to_token(history)\n",
        "    if input_ids.shape[1] > 2000:\n",
        "        history = [history[-1]]\n",
        "        input_ids = convert_history_to_token(history)\n",
        "    streamer = TextIteratorStreamer(tok, timeout=30.0, skip_prompt=True, skip_special_tokens=True)\n",
        "    generate_kwargs = dict(\n",
        "        input_ids=input_ids,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=temperature,\n",
        "        do_sample=temperature > 0.0,\n",
        "        top_p=top_p,\n",
        "        top_k=top_k,\n",
        "        repetition_penalty=repetition_penalty,\n",
        "        streamer=streamer,\n",
        "    )\n",
        "    if stop_tokens is not None:\n",
        "        generate_kwargs[\"stopping_criteria\"] = StoppingCriteriaList(stop_tokens)\n",
        "\n",
        "    stream_complete = Event()\n",
        "\n",
        "    def generate_and_signal_complete():\n",
        "        \"\"\"\n",
        "        genration function for single thread\n",
        "        \"\"\"\n",
        "        global start_time\n",
        "        ov_model.generate(**generate_kwargs)\n",
        "        stream_complete.set()\n",
        "\n",
        "    t1 = Thread(target=generate_and_signal_complete)\n",
        "    t1.start()\n",
        "\n",
        "    # Initialize an empty string to store the generated text\n",
        "    partial_text = \"\"\n",
        "    for new_text in streamer:\n",
        "        partial_text = text_processor(partial_text, new_text)\n",
        "        history[-1][1] = partial_text\n",
        "        yield history\n",
        "\n",
        "\n",
        "def request_cancel():\n",
        "    ov_model.request.cancel()\n",
        "\n",
        "\n",
        "def get_uuid():\n",
        "    \"\"\"\n",
        "    universal unique identifier for thread\n",
        "    \"\"\"\n",
        "    return str(uuid4())\n",
        "\n",
        "\n",
        "with gr.Blocks(\n",
        "    theme=gr.themes.Soft(),\n",
        "    css=\".disclaimer {font-variant-caps: all-small-caps;}\",\n",
        ") as demo:\n",
        "    conversation_id = gr.State(get_uuid)\n",
        "    gr.Markdown(f\"\"\"<h1><center>OpenVINO {model_id.value} Chatbot</center></h1>\"\"\")\n",
        "    chatbot = gr.Chatbot(height=500)\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            msg = gr.Textbox(\n",
        "                label=\"Chat Message Box\",\n",
        "                placeholder=\"Chat Message Box\",\n",
        "                show_label=False,\n",
        "                container=False,\n",
        "            )\n",
        "        with gr.Column():\n",
        "            with gr.Row():\n",
        "                submit = gr.Button(\"Submit\")\n",
        "                stop = gr.Button(\"Stop\")\n",
        "                clear = gr.Button(\"Clear\")\n",
        "    with gr.Row():\n",
        "        with gr.Accordion(\"Advanced Options:\", open=False):\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    with gr.Row():\n",
        "                        temperature = gr.Slider(\n",
        "                            label=\"Temperature\",\n",
        "                            value=0.1,\n",
        "                            minimum=0.0,\n",
        "                            maximum=1.0,\n",
        "                            step=0.1,\n",
        "                            interactive=True,\n",
        "                            info=\"Higher values produce more diverse outputs\",\n",
        "                        )\n",
        "                with gr.Column():\n",
        "                    with gr.Row():\n",
        "                        top_p = gr.Slider(\n",
        "                            label=\"Top-p (nucleus sampling)\",\n",
        "                            value=1.0,\n",
        "                            minimum=0.0,\n",
        "                            maximum=1,\n",
        "                            step=0.01,\n",
        "                            interactive=True,\n",
        "                            info=(\n",
        "                                \"Sample from the smallest possible set of tokens whose cumulative probability \"\n",
        "                                \"exceeds top_p. Set to 1 to disable and sample from all tokens.\"\n",
        "                            ),\n",
        "                        )\n",
        "                with gr.Column():\n",
        "                    with gr.Row():\n",
        "                        top_k = gr.Slider(\n",
        "                            label=\"Top-k\",\n",
        "                            value=50,\n",
        "                            minimum=0.0,\n",
        "                            maximum=200,\n",
        "                            step=1,\n",
        "                            interactive=True,\n",
        "                            info=\"Sample from a shortlist of top-k tokens — 0 to disable and sample from all tokens.\",\n",
        "                        )\n",
        "                with gr.Column():\n",
        "                    with gr.Row():\n",
        "                        repetition_penalty = gr.Slider(\n",
        "                            label=\"Repetition Penalty\",\n",
        "                            value=1.1,\n",
        "                            minimum=1.0,\n",
        "                            maximum=2.0,\n",
        "                            step=0.1,\n",
        "                            interactive=True,\n",
        "                            info=\"Penalize repetition — 1.0 to disable.\",\n",
        "                        )\n",
        "    gr.Examples(examples, inputs=msg, label=\"Click on any example and press the 'Submit' button\")\n",
        "\n",
        "    submit_event = msg.submit(\n",
        "        fn=user,\n",
        "        inputs=[msg, chatbot],\n",
        "        outputs=[msg, chatbot],\n",
        "        queue=False,\n",
        "    ).then(\n",
        "        fn=bot,\n",
        "        inputs=[\n",
        "            chatbot,\n",
        "            temperature,\n",
        "            top_p,\n",
        "            top_k,\n",
        "            repetition_penalty,\n",
        "            conversation_id,\n",
        "        ],\n",
        "        outputs=chatbot,\n",
        "        queue=True,\n",
        "    )\n",
        "    submit_click_event = submit.click(\n",
        "        fn=user,\n",
        "        inputs=[msg, chatbot],\n",
        "        outputs=[msg, chatbot],\n",
        "        queue=False,\n",
        "    ).then(\n",
        "        fn=bot,\n",
        "        inputs=[\n",
        "            chatbot,\n",
        "            temperature,\n",
        "            top_p,\n",
        "            top_k,\n",
        "            repetition_penalty,\n",
        "            conversation_id,\n",
        "        ],\n",
        "        outputs=chatbot,\n",
        "        queue=True,\n",
        "    )\n",
        "    stop.click(\n",
        "        fn=request_cancel,\n",
        "        inputs=None,\n",
        "        outputs=None,\n",
        "        cancels=[submit_event, submit_click_event],\n",
        "        queue=False,\n",
        "    )\n",
        "    clear.click(lambda: None, None, chatbot, queue=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run this to run app"
      ],
      "metadata": {
        "id": "j3YoXaxLh1wX"
      },
      "id": "j3YoXaxLh1wX"
    },
    {
      "cell_type": "code",
      "source": [
        "demo.launch()"
      ],
      "metadata": {
        "id": "xpQNJBakfyRy",
        "outputId": "59ed0832-9f14-44d3-ba25-e3453d8d6ba4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        }
      },
      "id": "xpQNJBakfyRy",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://8d7a1cc26485eb0594.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://8d7a1cc26485eb0594.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run this to close app"
      ],
      "metadata": {
        "id": "7_c3Y9Guh5HW"
      },
      "id": "7_c3Y9Guh5HW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b837f9e-4152-4a5c-880a-ed874aa64a74",
      "metadata": {
        "id": "7b837f9e-4152-4a5c-880a-ed874aa64a74",
        "outputId": "24d3d3d5-823e-4377-a90c-b9b6f85cdc0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closing server running on port: 7860\n"
          ]
        }
      ],
      "source": [
        "# please uncomment and run this cell for stopping gradio interface\n",
        "demo.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "22h89nAZfnIP"
      },
      "id": "22h89nAZfnIP",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "openvino_notebooks": {
      "imageUrl": "https://user-images.githubusercontent.com/29454499/255799218-611e7189-8979-4ef5-8a80-5a75e0136b50.png",
      "tags": {
        "categories": [
          "Model Demos",
          "AI Trends"
        ],
        "libraries": [],
        "other": [],
        "tasks": [
          "Text Generation",
          "Conversational"
        ]
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "66dd97760edb4f7294b940ce484064eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "English",
              "Chinese",
              "Japanese"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Model Language:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_11521864b77e4bdfb86e441e5e75ceb7",
            "style": "IPY_MODEL_e55dd0c3659a441e802913c067e94488"
          }
        },
        "11521864b77e4bdfb86e441e5e75ceb7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e55dd0c3659a441e802913c067e94488": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "31baeee064c24593b73ff90e0efa7220": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "qwen2-0.5b-instruct",
              "tiny-llama-1b-chat",
              "qwen2-1.5b-instruct",
              "gemma-2b-it",
              "red-pajama-3b-chat",
              "qwen2-7b-instruct",
              "gemma-7b-it",
              "llama-2-chat-7b",
              "llama-3-8b-instruct",
              "mpt-7b-chat",
              "mistral-7b",
              "zephyr-7b-beta",
              "notus-7b-v1",
              "neural-chat-7b-v3-1",
              "phi-3-mini-instruct"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Model:",
            "description_tooltip": null,
            "disabled": false,
            "index": 2,
            "layout": "IPY_MODEL_2dd331850c354933840ddc2552443c57",
            "style": "IPY_MODEL_0b97212db3b54a618e91bdba71b4fccb"
          }
        },
        "2dd331850c354933840ddc2552443c57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b97212db3b54a618e91bdba71b4fccb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc0964db45b74e7d80187c5d46ec76a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Prepare INT4 model",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_e306b160f6074a5c9901e472f0ea0e4a",
            "style": "IPY_MODEL_4ca07588c92e4c93aede06cd2460818f",
            "value": false
          }
        },
        "e306b160f6074a5c9901e472f0ea0e4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ca07588c92e4c93aede06cd2460818f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e92fab4be5f74c0198ebf8951d541174": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Prepare INT8 model",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_fe3ef60bd2064968a85edb51e832fd44",
            "style": "IPY_MODEL_f99518bbe9494318817749189068ef11",
            "value": false
          }
        },
        "fe3ef60bd2064968a85edb51e832fd44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f99518bbe9494318817749189068ef11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a4d2ca2fcca46f299063d2bd83c4fc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Prepare FP16 model",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_362e1881b74d46d3b31a045a86b60d83",
            "style": "IPY_MODEL_cf03cc071a7546cb8761000bfa123b4c",
            "value": true
          }
        },
        "362e1881b74d46d3b31a045a86b60d83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf03cc071a7546cb8761000bfa123b4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b079b23f0fd94df089ab7b3687bd5476": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Enable AWQ",
            "description_tooltip": null,
            "disabled": true,
            "indent": true,
            "layout": "IPY_MODEL_88fd63e0f8ed4e6a88e7f77296a2b32a",
            "style": "IPY_MODEL_fdc11a0fdd4c42d5bf5018f6603e2421",
            "value": false
          }
        },
        "88fd63e0f8ed4e6a88e7f77296a2b32a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fdc11a0fdd4c42d5bf5018f6603e2421": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7685bb0f3ebc479eb05b0cf05617f761": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "CPU",
              "AUTO"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Device:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_c803bca6d5b14e86a473c999805391ed",
            "style": "IPY_MODEL_f1f7e4310ce14928abfe1d9c05187a71"
          }
        },
        "c803bca6d5b14e86a473c999805391ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1f7e4310ce14928abfe1d9c05187a71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0010de2f731d4ce8af44bc6076fbb985": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "FP16"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Model to run:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_05f08b5b73494487990d74e1b7856af2",
            "style": "IPY_MODEL_4eeac02700bd4b1a802b3731a82f5b1a"
          }
        },
        "05f08b5b73494487990d74e1b7856af2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4eeac02700bd4b1a802b3731a82f5b1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}